#!/bin/python

from bs4 import BeautifulSoup
import os
from urllib.parse import urlparse, unquote, parse_qs, urlunparse, quote
from urllib.request import urlopen, Request
from shutil import copyfileobj
from quteprinter import qute_print, qute_wprint

DOWNLOAD_DIR = os.environ["HOME"] + "/Downloads"

def pixivExtractor(soup):
    user_tag = soup.select_one("aside > section > h2 a[href] > div:not([class])")
    user_name = None
    if user_tag is not None:
        user_name = user_tag.get_text(strip=True) or None

    def with_name(url):
        return (user_name, url) if user_name is not None else url

    def get_url(div):
        for x in div.select("a, img"):
            if "/img-original/" in (url := x.get("href", "")) or "/img-original/" in (url := x.get("src", "")):
                return with_name(url)
        return None

    highlighted = soup.select_one("body > div[role=presentation]")
    if highlighted is not None:
        return [get_url(highlighted)]

    if soup.select("#manga-viewer-close-anchor"):
        return [with_name(x["href"]) for x in soup.select("a[data-page]")]

    return [get_url(x) for x in soup.select("div[role=presentation] div[role=presentation]")]

def recceExtractor(soup):
    return [a.get("href") for a in soup.find_all("a") if a.get("href") is not None and a.get("href").endswith("_b.jpg")]

def gelbooruExtractor(soup):
    for a in soup.find_all("a"):
        if a.string == "Original image":
            return [a.get("href")]
    return []

def sankakucomplexExtractor(soup):
    # TODO: search for id directly instead
    for a in soup.find_all("a"):
        if a.get("id") == "highres":
            return [a.get("href")]
    return []

def chounyuuExtractor(soup):
    for i in soup.find_all("img"):
        # TODO: check if parent exists before accesing stuff from it
        if i.parent.name == "a" and i.parent.parent["class"] == ["image_box_image"]:
            return [i.get("src")]
    return []

def xfantazyExtractor(soup):
    # TODO: search for id directly instead
    for v in soup.find_all("video"):
        if v.get("id") == "fluid-videoplayer":
            return [v.get("src")]
    return []

def musesExtractor(soup):
    links = []
    for i in soup.find_all("img"):
        if i.get("class") == ["lazyloaded"]:
            # TODO: check if parent exists before accesing stuff from it
            filename = quote(i.parent.parent.get("title"))
            img = os.path.basename(i.get("src"))
            _, ext = os.path.splitext(img)
            links.append("/image/fm/{}?name={}{}".format(img, filename, ext))
    return links

def hentaifoundryExtractor(soup):
    if img := soup.select_one("div.boxbody > img"):
        return [img.get("src")]
    return []

def deviantartExtractor(soup):
    if (img := soup.select_one("img._3X6pY")) or (img := soup.select_one("img._2mdk5")):
        return [img.get("src")]
    return []

def name_from_url(url):
    up = urlparse(url)
    params = parse_qs(up.query)

    for n in ["name"]:
        if n in params:
            return unquote(params[n][0])

    return os.path.basename(unquote(up.path))

def make_url_absolute(host, url):
    u = urlparse(url)
    return urlunparse((
        u.scheme if u.scheme else "https",
        u.netloc if u.netloc else host,
        u.path,
        u.params,
        u.query,
        u.fragment
    ))

def main():
    extractors = {
        "recce.utn.se": recceExtractor,
        "gelbooru.com": gelbooruExtractor,
        "chan.sankakucomplex.com": sankakucomplexExtractor,
        "g.chounyuu.com": chounyuuExtractor,
        "xfantazy.com": xfantazyExtractor,
        "comics.8muses.com": musesExtractor,
        "www.hentai-foundry.com": hentaifoundryExtractor,
        "www.deviantart.com": deviantartExtractor,
        "www.pixiv.net": pixivExtractor,
    }

    std_header = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) QtWebEngine/5.15.2 Chrome/87.0.4280.144 Safari/537.36'}
    special_headers = {
        "www.pixiv.net": {"referer": "https://www.pixiv.net/"}
    }

    if "QUTE_HTML" not in os.environ:
        print("must be run from qutebrowser")
        exit(1)

    host = urlparse(os.environ["QUTE_URL"]).hostname

    extractor = extractors.get(host, None)

    if extractor is None:
        qute_wprint("site not supported")
        return

    with open(os.environ["QUTE_HTML"], "r") as fp:
        soup = BeautifulSoup(fp, "html5lib")

    image_urls = extractor(soup)

    if not image_urls:
        qute_wprint("didn't extract any images")
        return

    # qute_print("Downloading...")
    for u in image_urls:
        name_extra = None
        if isinstance(u, tuple):
            name_extra, u = u

        full_url = make_url_absolute(host, u)
        filename = name_from_url(full_url)
        if name_extra:
            filename = name_extra.replace('/', '-') + "_" + filename
        path = os.path.join(DOWNLOAD_DIR, filename)

        s_header = special_headers.get(host, {})
        r = Request(full_url, headers={**s_header, **std_header})
        with urlopen(r) as uopen, open(path, "wb") as f:
            copyfileobj(uopen, f)
            qute_print("Downloaded " + filename + "!")
    # qute_print("Done!")

if __name__ == "__main__":
   main()
